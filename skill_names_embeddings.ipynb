{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform preprocessing and generate word embeddings for all the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These files are bigger in size and i could not upload them to the GitHub repository mentioned in Thesis. Hence, i uploaded them in the submitted file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\" \n",
    "\n",
    "#Pre-trained en words embeddings file\n",
    "#https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "#enwiki_20180420 (window=5, iteration=10, negative=15): 300d (txt)\n",
    "pre_trained = path+\"enwiki_20180420_300d/enwiki_20180420_300d.txt\"\n",
    "#pre_trained = path+\"enwiki_20180420_100d/enwiki_20180420_100d.txt\"\n",
    "\n",
    "#Datasets\n",
    "\n",
    "# Dataset Assistment Skill Builder 2009 \n",
    "# totally contains 110 skills, 4.151 students, 446.844 records\n",
    "assist2009 = path+\"data/assist2009_updated/skill_builder_data.csv\"\n",
    "\n",
    "#Dataset Assistment Skill Builder 2009 skill builder corrected collapsed\n",
    "# totally contains 101 skills, 4.151 students, 274.590 records\n",
    "#https://sites.google.com/site/assistmentsdata/home/assistment-2009-2010-data/skill-builder-data-2009-2010\n",
    "assist_corrected = path+\"data/assistment_2009_corrected_3lines/skill_builder_data_corrected_collapsed.csv\"\n",
    "\n",
    "# Dataset Assistment 2012_2013_data_with_predictions\n",
    "# totally contains 196 skills, 28.834 students, 2.036.080 records\n",
    "#https://dl.acm.org/doi/10.1145/3079628.3079689\n",
    "#https://sites.google.com/site/assistmentsdata/home/2012-13-school-data-with-affect\n",
    "assist12_13 = path+\"data/assistment2012_2013/2012_2013_data_with_predictions.csv\"\n",
    "\n",
    "#Read dataset and  repair \"wrong words\" in skill names - \"wrong words\" are the for which wasn't found the embedding vector representation \n",
    "\n",
    "def read_data(DATASET=assist2009):\n",
    "    if DATASET==assist2009:\n",
    "        data = pd.read_csv(assist2009, delimiter=',', header=0, encoding='ANSI')\n",
    "        column_names = data.columns.tolist()\n",
    "        data = data[['user_id','skill_id','skill_name','correct']] \n",
    "        \n",
    "        # Replace multiply appears\n",
    "        data.loc[data['skill_id']==163,'skill_id']=85\n",
    "        \n",
    "        #Repair wrong words in skill names\n",
    "        data.loc[data['skill_name']=='Effect of Changing Dimensions of a Shape Prportionally','skill_name']='Effect of Changing Dimensions of a Shape Proportionally'\n",
    "        data.loc[data['skill_name']=='Parts of a Polyomial, Terms, Coefficient, Monomial, Exponent, Variable','skill_name']='Parts of a Polynomial Terms Coefficient Monomial Exponent Variable'\n",
    "        data.loc[data['skill_name']=='Order of Operations +,-,/,* () positive reals','skill_name']='Order of Operations addition subtraction division multiplication parentheses positive reals'\n",
    "        data.loc[data['skill_name']=='D.4.8-understanding-concept-of-probabilities','skill_name']='Understanding concept of probabilities'         \n",
    "        data.loc[data['skill_name']=='Angles - Obtuse, Acute, and Right','skill_name']='Angles Obtuse Acute and Right'        \n",
    "               \n",
    "        #Drop the rows where at least one element is missing.\n",
    "        data = data.dropna()\n",
    "        skills = np.array(data['skill_name'].unique())\n",
    "        skill_ids = np.array(data['skill_id'].unique()).astype(int)\n",
    "        students = np.array(data['user_id'].unique())\n",
    "        data = data.astype({\"skill_id\": int})\n",
    "    elif DATASET==assist_corrected:\n",
    "        data= pd.read_csv(assist_corrected, delimiter=',', header=0, encoding='ANSI')\n",
    "        column_names = data.columns.tolist()\n",
    "        data = data[['user_id','skill_id','skill_name','correct']]\n",
    "        \n",
    "        # Replace multiply appears \n",
    "        data.loc[data['skill_id']=='163','skill_id']='85'\n",
    "        data.loc[data['skill_id']=='1_13','skill_id']='1'\n",
    "        data.loc[data['skill_id']=='1_15','skill_id']='1'\n",
    "        data.loc[data['skill_id']=='10_104','skill_id']='10'\n",
    "        data.loc[data['skill_id']=='10_12','skill_id']='10'\n",
    "        data.loc[data['skill_id']=='10_13','skill_id']='10'\n",
    "        data.loc[data['skill_id']=='10_14','skill_id']='10'\n",
    "        data.loc[data['skill_id']=='10_15','skill_id']='10'\n",
    "        data.loc[data['skill_id']=='10_18','skill_id']='10'\n",
    "        data.loc[data['skill_id']=='10_58','skill_id']='10'\n",
    "        data.loc[data['skill_id']=='10_64','skill_id']='10'\n",
    "        data.loc[data['skill_id']=='11_70','skill_id']='11'\n",
    "        data.loc[data['skill_id']=='16_17','skill_id']='16'\n",
    "        data.loc[data['skill_id']=='173_190_193_221','skill_id']='170'\n",
    "        data.loc[data['skill_id']=='2_37_48','skill_id']='2'\n",
    "        data.loc[data['skill_id']=='2_37_70','skill_id']='2'\n",
    "        data.loc[data['skill_id']=='2_37_48_77','skill_id']='2'\n",
    "        data.loc[data['skill_id']=='2_48_79','skill_id']='2'\n",
    "        data.loc[data['skill_id']=='2_70','skill_id']='2'\n",
    "        data.loc[data['skill_id']=='21_22','skill_id']='21'\n",
    "        data.loc[data['skill_id']=='24_46','skill_id']='24'\n",
    "        data.loc[data['skill_id']=='27_321','skill_id']='27'\n",
    "        data.loc[data['skill_id']=='292_293','skill_id']='292'\n",
    "        data.loc[data['skill_id']=='34_69','skill_id']='34'\n",
    "        data.loc[data['skill_id']=='34_81','skill_id']='34'\n",
    "        data.loc[data['skill_id']=='34_278','skill_id']='34'\n",
    "        data.loc[data['skill_id']=='35_46','skill_id']='35'\n",
    "        data.loc[data['skill_id']=='46_79','skill_id']='46'\n",
    "        data.loc[data['skill_id']=='47_63_70_77','skill_id']='47'\n",
    "        data.loc[data['skill_id']=='48_63_70_79','skill_id']='48'\n",
    "        data.loc[data['skill_id']=='48_63_79','skill_id']='48'\n",
    "        data.loc[data['skill_id']=='49_50','skill_id']='49'\n",
    "        data.loc[data['skill_id']=='5_102','skill_id']='5'\n",
    "        data.loc[data['skill_id']=='5_375','skill_id']='5'  \n",
    "        data.loc[data['skill_id']=='51_53_75','skill_id']='51'\n",
    "        data.loc[data['skill_id']=='58_85','skill_id']='58'\n",
    "        data.loc[data['skill_id']=='63_75','skill_id']='63'\n",
    "        data.loc[data['skill_id']=='74_85','skill_id']='74'\n",
    "        data.loc[data['skill_id']=='74_92','skill_id']='74'\n",
    "        data.loc[data['skill_id']=='9_12','skill_id']='9'\n",
    "        data.loc[data['skill_id']=='9_13','skill_id']='9'\n",
    "        data.loc[data['skill_id']=='9_14','skill_id']='9'\n",
    "        data.loc[data['skill_id']=='9_15','skill_id']='9'\n",
    "        data.loc[data['skill_id']=='92_323','skill_id']='92'\n",
    "        data.loc[data['skill_id']=='97_99_105','skill_id']='97' \n",
    "        data.loc[data['skill_id']==307,'skill_id']='307'\n",
    "        \n",
    "        #Repair wrong words in skill names\n",
    "        data.loc[data['skill_name']=='Effect of Changing Dimensions of a Shape Prportionally','skill_name']='Effect of Changing Dimensions of a Shape Proportionally'\n",
    "        data.loc[data['skill_name']=='Parts of a Polyomial, Terms, Coefficient, Monomial, Exponent, Variable','skill_name']='Parts of a Polynomial Terms Coefficient Monomial Exponent Variable'\n",
    "        data.loc[data['skill_name']=='Order of Operations +,-,/,* () positive reals','skill_name']='Order of Operations addition subtraction division multiplication parentheses positive reals'\n",
    "        data.loc[data['skill_name']=='D.4.8-understanding-concept-of-probabilities','skill_name']='Understanding concept of probabilities'         \n",
    "        data.loc[data['skill_name']=='Angles - Obtuse, Acute, and Right','skill_name']='Angles Obtuse Acute and Right'   \n",
    "\n",
    "        #Drop the rows where at least one element is missing.\n",
    "        data = data.dropna()\n",
    "        skills = np.array(data['skill_name'].unique())\n",
    "        skill_ids = np.array(data['skill_id'].unique()).astype(int)\n",
    "        students = np.array(data['user_id'].unique())\n",
    "        data = data.astype({\"skill_id\": int})\n",
    "        \n",
    "    elif DATASET==assist12_13:\n",
    "        data = pd.read_csv(assist12_13, delimiter=',', header=0)\n",
    "        column_names = data.columns.tolist()\n",
    "        data = data[['user_id','skill_id','skill','problem_id','correct']]\n",
    "                \n",
    "        # Replace multiply appears\n",
    "        data.loc[data['skill_id']==163,'skill_id']=85\n",
    "        data.loc[data['skill_id']==613,'skill_id']=590\n",
    "        data.loc[data['skill_id']==612,'skill_id']=589\n",
    "        data.loc[data['skill_id']==571,'skill_id']=39\n",
    "        data.loc[data['skill_id']==595,'skill_id']=39\n",
    "        data.loc[data['skill_id']==578,'skill_id']=295\n",
    "        data.loc[data['skill_id']==601,'skill_id']=295\n",
    "        data.loc[data['skill_id']==576,'skill_id']=296\n",
    "        data.loc[data['skill_id']==600,'skill_id']=296\n",
    "        data.loc[data['skill_id']==570,'skill_id']=297\n",
    "        data.loc[data['skill_id']==594,'skill_id']=297\n",
    "        data.loc[data['skill_id']==579,'skill_id']=298\n",
    "        data.loc[data['skill_id']==602,'skill_id']=298\n",
    "        data.loc[data['skill_id']==604,'skill_id']=581\n",
    "        data.loc[data['skill_id']==174,'skill_id']=2\n",
    "        data.loc[data['skill_id']==572,'skill_id']=40\n",
    "        data.loc[data['skill_id']==596,'skill_id']=40\n",
    "        data.loc[data['skill_id']==599,'skill_id']=575\n",
    "        data.loc[data['skill_id']==597,'skill_id']=573\n",
    "        data.loc[data['skill_id']==608,'skill_id']=585\n",
    "        data.loc[data['skill_id']==588,'skill_id']=48\n",
    "        data.loc[data['skill_id']==611,'skill_id']=48\n",
    "        data.loc[data['skill_id']==181,'skill_id']=86\n",
    "        data.loc[data['skill_id']==580,'skill_id']=86\n",
    "        data.loc[data['skill_id']==603,'skill_id']=86\n",
    "        data.loc[data['skill_id']==1641,'skill_id']=218\n",
    "        data.loc[data['skill_id']==390,'skill_id']=106\n",
    "        data.loc[data['skill_id']==586,'skill_id']=317\n",
    "        data.loc[data['skill_id']==609,'skill_id']=317\n",
    "        data.loc[data['skill_id']==186,'skill_id']=4\n",
    "        data.loc[data['skill_id']==610,'skill_id']=65\n",
    "        data.loc[data['skill_id']==587,'skill_id']=65\n",
    "        data.loc[data['skill_id']==196,'skill_id']=12\n",
    "        data.loc[data['skill_id']==198,'skill_id']=13\n",
    "        data.loc[data['skill_id']==606,'skill_id']=583\n",
    "        data.loc[data['skill_id']==605,'skill_id']=582\n",
    "        data.loc[data['skill_id']==607,'skill_id']=584\n",
    "        data.loc[data['skill_id']==202,'skill_id']=92\n",
    "        data.loc[data['skill_id']==212,'skill_id']=79\n",
    "        data.loc[data['skill_id']==213,'skill_id']=27\n",
    "        data.loc[data['skill_id']==216,'skill_id']=15\n",
    "        data.loc[data['skill_id']==375,'skill_id']=222\n",
    "        data.loc[data['skill_id']==226,'skill_id']=95\n",
    "        data.loc[data['skill_id']==598,'skill_id']=574\n",
    "        data.loc[data['skill_id']==614,'skill_id']=591\n",
    "        data.loc[data['skill_id']==569,'skill_id']=301\n",
    "        data.loc[data['skill_id']==593,'skill_id']=301\n",
    "        data.loc[data['skill_id']==568,'skill_id']=307\n",
    "        data.loc[data['skill_id']==592,'skill_id']=307\n",
    "        \n",
    "        #Repair wrong words in skill names\n",
    "        data.loc[data['skill']=='Effect of Changing Dimensions of a Shape Prportionally','skill']='Effect of Changing Dimensions of a Shape Proportionally'\n",
    "        data.loc[data['skill']=='Parts of a Polyomial, Terms, Coefficient, Monomial, Exponent, Variable','skill']='Parts of a Polynomial Terms Coefficient Monomial Exponent Variable'\n",
    "        data.loc[data['skill']=='Order of Operations +,-,/,* () positive reals','skill']='Order of Operations addition subtraction division multiplication parentheses positive reals'\n",
    "        data.loc[data['skill']=='D.4.8-understanding-concept-of-probabilities','skill']='Understanding concept of probabilities'         \n",
    "        data.loc[data['skill']=='Angles - Obtuse, Acute, and Right','skill']='Angles Obtuse Acute and Right'         \n",
    "        data.loc[data['skill']=='Expanded, Standard and Word Notation','skill']='Expanded Standard and Word Notation'\n",
    "        data.loc[data['skill']=='Tree Diagrams, Lists for Counting','skill']='Tree Diagrams Lists for Counting'\n",
    "        data.loc[data['skill']=='Finding y-intercept from Linear Equation','skill']='Finding y intercept from Linear Equation'\n",
    "        data.loc[data['skill']=='Write Linear Equation from Slope and y-intercept','skill']='Write Linear Equation from Slope and y intercept'\n",
    "        data.loc[data['skill']=='X-Y Graph Reading','skill']='X Y Graph Reading'        \n",
    "        data.loc[data['skill']=='Comparing and Identifying Slope/Rate of Change','skill']='Comparing and Identifying Slope Rate of Change'                 \n",
    "        data.loc[data['skill']=='Writine Expression from Diagrams','skill']='Writing Expression from Diagrams'        \n",
    "        data.loc[data['skill']=='Calculation with + - * /','skill']='Calculation with addition subtraction multiplication division'        \n",
    "        data.loc[data['skill']=='Line of Best-Fit','skill']='Line of Best Fit'        \n",
    "        data.loc[data['skill']=='Properties and Clasification of Pyramid','skill']='Properties and Classification of Pyramid'        \n",
    "        data.loc[data['skill']=='Mean-Median-Mode-Range Differentiation','skill']='Mean Median Mode Range Differentiation'\n",
    "        data.loc[data['skill']=='Co-ordinate Points','skill']='Coordinate Points'\n",
    "        data.loc[data['skill']=='Circumference ','skill']='Circumference'\n",
    "        data.loc[data['skill']=='Pattern Finding ','skill']='Pattern Finding'\n",
    "        \n",
    "        #Drop the rows where at least one element is missing.\n",
    "        data = data.dropna()\n",
    "        skills = np.array(data['skill'].unique())\n",
    "        skill_ids = np.array(data['skill_id'].unique()).astype(int)\n",
    "        problems = np.array(data['problem_id'].unique())\n",
    "        students = np.array(data['user_id'].unique())\n",
    "        data = data.astype({\"skill_id\": int})\n",
    "        data = data.astype({\"correct\": int})\n",
    "    else:\n",
    "        print(\"No Dataset: {}\".format(DATASET), file=sys.stderr)\n",
    "    return data, skills, skill_ids, students\n",
    "\n",
    "#split list of words\n",
    "def convert(lst):\n",
    "    return (lst.split())\n",
    "\n",
    "DATASET=assist_corrected\n",
    "dataset_file, skills, skill_ids, students = read_data(DATASET=DATASET)\n",
    "\n",
    "\n",
    "vec_arr=[] \n",
    "word_arr=[]\n",
    "not_found=[]\n",
    "in_skill_name=[]\n",
    "s = skills.shape[0] \n",
    "d = 300\n",
    "i = 0\n",
    "counter = 0\n",
    "sentence_emb=np.zeros([s,d])\n",
    "sentence_emb[0,:]=0.0\n",
    "for i in range(s):\n",
    "    low_words=skills[i].lower()\n",
    "    skills_low=convert(low_words)\n",
    "    vectors=np.zeros\n",
    "    #words=np.array()\n",
    "    print('Skill name:{} - {}'.format(i, skills[i]))\n",
    "    for word in skills_low:        \n",
    "        with open(pre_trained, 'r') as f:  #the format of pre_trained file is : word number1 number2 number3 .........number300 - one line for each word, tab delimited\n",
    "            while True:\n",
    "                counter += 1\n",
    "                try:\n",
    "                    \n",
    "                    line = f.readline()\n",
    "                    if not line:\n",
    "                        not_found.append(word)  #finding the word that doesn't exist in pre-treied file\n",
    "                        in_skill_name.append(skills[i])  #finding the skill name in which one or more words have not been found\n",
    "                        break\n",
    "                    if counter % 10000000 == 0 :\n",
    "                        print ('line = {}'.format(line))\n",
    "                    if word+\" \"==line[:len(word)+1]:\n",
    "                        str_list = line[len(word)+1:].split(\" \")\n",
    "                        vec = [float(num) for num in str_list]                        \n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    e = None\n",
    "                    #print(\"Exception {}\".format(e))\n",
    "            vec_arr.append(vec)\n",
    "            word_arr.append(word)         \n",
    "            print(\"{}\".format(word))               \n",
    "    vectors=np.array(vec_arr) \n",
    "    vec_arr.clear()\n",
    "    words=np.array(word_arr)\n",
    "    not_found_words=np.array(not_found)\n",
    "    no_word_in_skill=np.array(in_skill_name)\n",
    "    sentence_emb[i,:] = np.sum(vectors, axis=0) #skill name embedding vector is the sum of the words embeddings of the skill name \n",
    "f.close() \n",
    "\n",
    "# Store the 4 columns of dataset, skill names, skill names embeddings, not founded words and skill names with not founded words \n",
    "if DATASET==assist2009:  \n",
    "    pd.DataFrame(dataset_file).to_csv(path+\"data/assist2009_updated/assist2009_wrong_data.csv\",sep=\",\", index=False)\n",
    "    pd.DataFrame(sentence_emb).to_csv(path+\"data/assist2009_updated/skill_name_embeddings2009.csv\",sep=\",\", index=False, header=None)   \n",
    "    pd.DataFrame(skills).to_csv(path+\"data/assist2009_updated/skill_names2009.csv\",sep=\",\", index=False, header=None)\n",
    "    pd.DataFrame(not_found_words).to_csv(path+\"not_found_words2009.csv\", sep=\",\", index=False, header=None)\n",
    "    pd.DataFrame(no_word_in_skill).to_csv(path+\"no_words_in_skill2009.csv\", sep=\",\", index=False, header=None)\n",
    "    \n",
    "elif DATASET==assist_corrected:  \n",
    "    dataset_file.to_csv(path+\"data/assistment_2009_corrected_3lines/assistment2009_corrected.csv\",sep=\",\", index=False)    \n",
    "    pd.DataFrame(sentence_emb).to_csv(path+\"data/assistment_2009_corrected_3lines/skill_name_embeddings_corrected.csv\",sep=\",\", index=False, header=None)   \n",
    "    pd.DataFrame(skills).to_csv(path+\"data/assistment_2009_corrected_3lines/skill_names_corrected.csv\",sep=\",\", index=False, header=None)\n",
    "    pd.DataFrame(not_found_words).to_csv(path+\"not_found_words_corrected.csv\", sep=\",\", index=False, header=None)\n",
    "    pd.DataFrame(no_word_in_skill).to_csv(path+\"no_words_in_skill_corrected.csv\", sep=\",\", index=False, header=None)\n",
    "\n",
    "elif DATASET==assist12_13:  \n",
    "    dataset_file.to_csv(path+\"data/assistment2012_2013/assistment2012_13.csv\",sep=\",\", index=False)\n",
    "    pd.DataFrame(sentence_emb).to_csv(path+\"data/assistment2012_2013/skill_name_embeddings_12_13.csv\",sep=\",\", index=False, header = None)   \n",
    "    pd.DataFrame(skills).to_csv(path+\"data/assistment2012_2013/skill_names_12_13.csv\",sep=\",\", index=False, header=None)\n",
    "    pd.DataFrame(not_found_words).to_csv(path+\"data/assistment2012_2013/not_found_words_12_13.csv\", sep=\",\", index=False, header=None)\n",
    "    pd.DataFrame(no_word_in_skill).to_csv(path+\"data/assistment2012_2013/no_words_in_skill_12_13.csv\", sep=\",\", index=False, header=None)\n",
    "\n",
    "else:\n",
    "    print(\"No Dataset: {}\".format(DATASET), file=sys.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
