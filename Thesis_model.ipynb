{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis model\n",
    "\n",
    "## Notes:\n",
    "### 1. The commented lines in the cells named as 'Read csv files' and 'Train model' train the model in different datasets and perform validation and/or testing, respectively.\n",
    "\n",
    "### 2. The global variables in the cell 'Set Global variables' define the emb size, the use of Word2vec or FastText embeddings, the training in the 70% and other parameters to train the thesis model.\n",
    "\n",
    "## How to run the model:\n",
    "\n",
    "### If you want to run the embeddings of 300 dimensions you need to set:\n",
    "### 1. 'emb_size=300'\n",
    "### 2. 'w2v_emb_size=300'\n",
    "### 3. 'filters=300' in Conv1D layer. \n",
    "### Uncomment also the respective embeddings of 300dimensions.\n",
    "\n",
    "## The example below is for training the model in the 'assistment_2009_corrected_3lines' dataset with Word2Vec embeddings of 100dimensions. After training, execute the cell with name 'evaluate', to evaluate the model in the test set.\n",
    "\n",
    "## If need to test the performance in a validation set, you need to:\n",
    "### 1. set 'train_all=False'\n",
    "### 2. change the fold number in 'Read csv files' cell to read the corresponding split.\n",
    "### 3. Uncomment the lines 37-39 and 49-77 in 'Train model' cell.\n",
    "### 4.Execute the cells 'Calculate mean validation AUC', 'Calculate mean validation accuracy', 'Calculate mean validation accuracy' to measure its performance in AUC, accuracy and loss respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print('The current directory is: ', os.getcwd())\n",
    "import numpy as np\n",
    "print('The numpy version is: ', np.version.version)\n",
    "import pandas as pd\n",
    "print('The pandas version is: ', pd.__version__)\n",
    "!pip install tensorflow==2.1.0\n",
    "!pip install tensorflow-gpu==2.1.0\n",
    "# %tensorflow_version 2.1.0\n",
    "import tensorflow as tf\n",
    "print('The tensorflow version is: ', tf.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import toeplitz\n",
    "\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Input, Embedding, Concatenate, Activation, Dense, \\\n",
    "                                    Flatten, LSTM, SpatialDropout1D, Dropout, GRU, Bidirectional, \\\n",
    "                                    Lambda, Multiply, Permute, RepeatVector, Masking, TimeDistributed, \\\n",
    "                                    Attention, AdditiveAttention, Conv1D, MaxPool1D, AveragePooling1D, GlobalAveragePooling1D, \\\n",
    "                                    BatchNormalization, Activation, LocallyConnected1D\n",
    "                                                           \n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam, Adamax\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import AUC, binary_crossentropy\n",
    "from tensorflow.keras.initializers import Constant, RandomUniform\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "# GPU information\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE_W2V = False\n",
    "USE_W2V = True\n",
    "\n",
    "# train_all = False\n",
    "train_all = True\n",
    "\n",
    "emb_size = 100 # skill embedding size\n",
    "w2v_emb_size = 100 \n",
    "L = 50 #10 #20 # history length\n",
    "max_epochs = 20 # training epochs\n",
    "beta = 1e-3 # learning rate\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create progress bar to monitor the reading of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printProgressBar (iteration, total, prefix = '', suffix = '',\n",
    "                      decimals = 1, length = 100, fill = 'β–', printEnd = \"\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = printEnd, flush=True)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read 3 lines format of csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_3lines(file, start_user):\n",
    "    user_ids = []\n",
    "    skill_ids = []\n",
    "    correct = []\n",
    "    with open(file, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        cnt = 0\n",
    "        user_id = start_user\n",
    "        try:\n",
    "            num_responses = int(line)\n",
    "        except:\n",
    "            print('Error')\n",
    "        user_ids += [user_id]*num_responses\n",
    "        while line:\n",
    "            line = f.readline()\n",
    "            if line==\"\":\n",
    "                break\n",
    "            cnt += 1\n",
    "            if cnt%3 == 0:\n",
    "                user_id += 1\n",
    "                num_responses = int(line)\n",
    "                user_ids += [user_id]*num_responses\n",
    "            elif cnt%3 == 1:\n",
    "                skill_ids += line.replace(\"\\n\",\"\").split(\",\")\n",
    "            elif cnt%3==2:\n",
    "                correct += line.replace(\"\\n\",\"\").split(\",\")\n",
    "        user_ids = np.reshape(np.array(user_ids),[-1,1])\n",
    "        num_unique_users = np.unique(user_ids[:,0]).shape[0]\n",
    "        skill_ids = np.reshape(np.array(skill_ids).astype(int),[-1,1])\n",
    "        correct = np.reshape(np.array(correct).astype(int),[-1,1])\n",
    "        idx = np.reshape((correct==0) + (correct==1), [-1])\n",
    "        data = np.hstack((user_ids[idx], skill_ids[idx], correct[idx]))\n",
    "        return data, num_unique_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    # use_all_train\n",
    "    if train_all:\n",
    "        print('I use all the training: ')\n",
    "        train_file = \"./data/assistment_2009_corrected_3lines/assistment_2009_corrected_train.csv\"\n",
    "        valid_file = \"./data/assistment_2009_corrected_3lines/assistment_2009_corrected_valid1.csv\"\n",
    "        test_file = \"./data/assistment_2009_corrected_3lines/assistment_2009_corrected_test.csv\"\n",
    "        # train_file = \"./data/assist2009_updated/assist2009_updated_train.csv\"\n",
    "        # valid_file = \"./data/assist2009_updated/assist2009_updated_valid0.csv\"\n",
    "        # test_file = \"./data/assist2009_updated/assist2009_updated_test.csv\"\n",
    "        # train_file = \"./data/fsaid1tof3/fsaif1tof3_train.csv\"\n",
    "        # valid_file = \"./data/fsaid1tof3/fsaif1tof3_valid1.csv\"\n",
    "        # test_file = \"./data/fsaid1tof3/fsaif1tof3_test.csv\"\n",
    "        # train_file = \"./data/assistment2012_2013/assistment2012_13_train.csv\"\n",
    "        # valid_file = \"./data/assistment2012_2013/assistment2012_13_valid1.csv\"\n",
    "        # test_file = \"./data/assistment2012_2013/assistment2012_13_test.csv\"\n",
    "    else:\n",
    "        train_file = \"./data/assistment_2009_corrected_3lines/assistment_2009_corrected_train1.csv\"\n",
    "        valid_file = \"./data/assistment_2009_corrected_3lines/assistment_2009_corrected_valid1.csv\"\n",
    "        test_file = \"./data/assistment_2009_corrected_3lines/assistment_2009_corrected_test.csv\"\n",
    "        # train_file = \"./data/assist2009_updated/assist2009_updated_train4.csv\"\n",
    "        # valid_file = \"./data/assist2009_updated/assist2009_updated_valid4.csv\"\n",
    "        # test_file = \"./data/assist2009_updated/assist2009_updated_test.csv\"\n",
    "        # train_file = \"./data/fsaid1tof3/fsaif1tof3_train1.csv\"\n",
    "        # valid_file = \"./data/fsaid1tof3/fsaif1tof3_valid1.csv\"\n",
    "        # test_file = \"./data/fsaid1tof3/fsaif1tof3_test.csv\"\n",
    "        # train_file = \"./data/assistment2012_2013/assistment2012_13_train1.csv\"\n",
    "        # valid_file = \"./data/assistment2012_2013/assistment2012_13_valid1.csv\"\n",
    "        # test_file = \"./data/assistment2012_2013/assistment2012_13_test.csv\"\n",
    "\n",
    "    # Read skill names\n",
    "    sknames_file = \"./data/assistment_2009_corrected_3lines/skill_names_corrected.csv\"\n",
    "    # sknames_file = \"./data/assist2009_updated/skill_names_updated.csv\"\n",
    "    # sknames_file = \"./data/fsaif1tof3/fsaif1tof3_skill_name_question_id.csv\"\n",
    "    # sknames_file = \"./data/assistment2012_2013/skill_names_12_13.csv\"\n",
    "\n",
    "    skill_names = pd.read_csv(sknames_file, header=None).values\n",
    "    \n",
    "    # Read embedding data\n",
    "    emb_file = './embeddings/assistment_2009_corrected_3lines/skill_name_embeddings_corrected_100d.csv'\n",
    "    # emb_file = './embeddings/assistment_2009_corrected_3lines/skill_name_embeddings_corrected_300d.csv'\n",
    "    # emb_file = './embeddings/assistment_2009_corrected_3lines/Assistment2009_corrected_skname_embeddings_FastText.csv'\n",
    "\n",
    "    # emb_file = './embeddings/assist2009_updated/skill_name_embeddings_updated100d.csv'\n",
    "    # emb_file = './embeddings/assist2009_updated/skill_name_embeddings_updated300d.csv'\n",
    "    # emb_file = './embeddings/assist2009_updated/Assist2009_updated_skname_embeddings_FastText.csv'\n",
    "    \n",
    "    # emb_file = './embeddings/fsaif1tof3/fsaif1tof3_embeddings_100d.csv'\n",
    "    # emb_file = './embeddings/fsaif1tof3/fsaif1tof3_embeddings_300d.csv'\n",
    "    # emb_file = './embeddings/fsaif1tof3/fsaif1tof3_skname_embeddings_FastText.csv'\n",
    "    \n",
    "    # emb_file = \"./embeddings/assistment2012_2013/skill_name_embeddings_12_13_100d.csv\"\n",
    "    # emb_file = \"./embeddings/assistment2012_2013/skill_name_embeddings_12_13_300d.csv\"\n",
    "    # emb_file = \"./embeddings/assistment2012_2013/Assistment2012_13_skname_embeddings_FastText.csv\"\n",
    "    \n",
    "    embeddings = pd.read_csv(emb_file, header=None)\n",
    "    \n",
    "    # Add a zero row at the beginning\n",
    "    emb_size = embeddings.shape[1]\n",
    "    print('emb size: ', emb_size)\n",
    "    embeddings = np.vstack((np.zeros([1,emb_size]), embeddings)) # if i put 1 i add 0 in the first line\n",
    "    \n",
    "    \"\"\"\n",
    "    Read Train, Validation, Test File\n",
    "    \"\"\"\n",
    "    start_user = 1\n",
    "    data_train, N_train = read_file_3lines(train_file, start_user)\n",
    "    start_user += N_train\n",
    "    data_valid, N_valid = read_file_3lines(valid_file, start_user)\n",
    "    start_user += N_valid\n",
    "    data_test, N_test = read_file_3lines(test_file, start_user)\n",
    "    return data_train, data_test, data_valid, embeddings, skill_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, data_valid, embeddings, skill_names = read_data()\n",
    "print('train:',data_train, '\\ntest:',data_test, '\\nvalid:', data_valid)\n",
    "print('train:', data_train.shape, 'test:', data_test.shape, 'valid:', data_valid.shape)\n",
    "print('\\nembeddings:', embeddings)\n",
    "print('shape after adding 0 in the first line: ', embeddings.shape)\n",
    "skill_ids = np.unique(np.hstack((\n",
    "    data_train[:,1],\n",
    "    data_valid[:,1],\n",
    "    data_test[:,1]\n",
    ")))\n",
    "print('The unique skill ids from all train, test, valid are: ', skill_ids) \n",
    "\n",
    "num_skills = len(skill_ids)\n",
    "train_user_ids = np.unique(data_train[:,0])\n",
    "valid_user_ids = np.unique(data_valid[:,0])\n",
    "test_user_ids = np.unique(data_test[:,0])\n",
    "N_train = len(train_user_ids)\n",
    "N_valid = len(valid_user_ids)\n",
    "N_test = len(test_user_ids)\n",
    "num_students = N_train + N_test + N_valid\n",
    "print('Number of skills: {}'.format(num_skills))\n",
    "print('Number of train students: {}'.format(N_train))\n",
    "print('Number of validation students: {}'.format(N_valid))\n",
    "print('Number of test students: {}'.format(N_test))\n",
    "print('(total students: {})'.format(num_students))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data to desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_inputs_targets(data, user_ids, N, prefix):\n",
    "    printProgressBar(0, N, prefix = prefix, suffix = 'Complete', length = 50)\n",
    "    \n",
    "    x = None\n",
    "    t = None\n",
    "    start = True\n",
    "    for i, student_id in enumerate(user_ids):\n",
    "        # Make an array with all the data for this student\n",
    "        student_data = data[data[:,0]==student_id]\n",
    "        skill_hist = toeplitz(student_data[:,1],0.0*np.ones([1,L]))\n",
    "        responses_hist = toeplitz(student_data[:,2],0.0*np.ones([1,L]))\n",
    "        student_data = np.hstack((skill_hist,\n",
    "                                np.fliplr(responses_hist)\n",
    "                                ))\n",
    "        if start:\n",
    "            start = False\n",
    "            x = student_data[1:,0:2*L-1]\n",
    "            t = student_data[1:,2*L-1].reshape([-1,1])\n",
    "        else:\n",
    "            x = np.vstack((x, student_data[1:,0:2*L-1]))\n",
    "            t = np.vstack((t, student_data[1:,2*L-1].reshape([-1,1])))\n",
    "        printProgressBar(i+1, N, prefix = prefix, suffix = 'Complete', length = 50)        \n",
    "    return x, t\n",
    "\n",
    "x_train, t_train = gen_inputs_targets(data_train,\n",
    "                            train_user_ids, N_train, 'Train set:')\n",
    "print('x_train:', x_train.shape, 'train responses:', t_train.shape)\n",
    "x_valid, t_valid = gen_inputs_targets(data_valid,\n",
    "                        valid_user_ids, N_valid, 'Validation set:')\n",
    "print('x_valid:', x_valid.shape, 'valid responses:', t_valid.shape)\n",
    "\n",
    "x_test, t_test = gen_inputs_targets(data_test,\n",
    "                        test_user_ids, N_test, 'Test set:')\n",
    "print('x_test:', x_test.shape, 'test responses:', t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create thesis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thesis_model():\n",
    "    # Inputs\n",
    "    q_ids = Input(shape=[L], dtype=tf.int32)\n",
    "    hist_ids = Input(shape=[L-1], dtype=tf.int32)\n",
    "    if USE_W2V:\n",
    "        print('I use embeddings:')\n",
    "        initial_h_emb = RandomUniform(minval=-1/(w2v_emb_size*L),maxval=1/(w2v_emb_size*L))\n",
    "        hist = Embedding(2, w2v_emb_size,\n",
    "                    embeddings_initializer=initial_h_emb, trainable=True)(hist_ids)\n",
    "        hist = SpatialDropout1D(0.2)(hist)\n",
    "\n",
    "        initial_q_emb = Constant(embeddings/(L*w2v_emb_size))\n",
    "        print(embeddings)\n",
    "        print(embeddings/(L*w2v_emb_size))\n",
    "        q = Embedding(embeddings.shape[0], w2v_emb_size,\n",
    "                    embeddings_initializer=initial_q_emb, trainable=True)(q_ids)\n",
    "        q = SpatialDropout1D(0.2)(q)\n",
    "    else:\n",
    "        print('I do not use embeddings:')\n",
    "        initial_h_emb = RandomUniform(minval=-1/(emb_size*L),maxval=1/(emb_size*L))\n",
    "        hist = Embedding(2, emb_size,\n",
    "                    embeddings_initializer=initial_h_emb, trainable=True)(hist_ids)\n",
    "        hist = SpatialDropout1D(0.2)(hist)\n",
    "        \n",
    "        initial_q_emb = RandomUniform(minval=-1/(emb_size*L),maxval=1/(emb_size*L))\n",
    "        q = Embedding(embeddings.shape[0], emb_size,\n",
    "                    embeddings_initializer=initial_q_emb, trainable=True)(q_ids)\n",
    "        q = SpatialDropout1D(0.2)(q)\n",
    "\n",
    "    q_conv = Conv1D(filters=100, kernel_size=3, strides=1, activation=\"relu\")(q)\n",
    "\n",
    "    merged = Concatenate(axis=1)([q_conv, hist])\n",
    "\n",
    "    x_lstm = Bidirectional(GRU(units=64, return_sequences=False))(merged)\n",
    "\n",
    "    x_dropout = Dropout(0.2)(x_lstm)\n",
    "\n",
    "    d1 = Dense(50, activation='relu')(x_dropout)\n",
    "\n",
    "    d2 = Dense(25, activation='relu')(d1)\n",
    "\n",
    "    x = Dense(1, activation='sigmoid')(d2)\n",
    "    \n",
    "    model = Model(inputs=[q_ids, hist_ids], outputs=x)\n",
    "\n",
    "    # visualize the model\n",
    "    tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"./thesis_model.png\",\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    ")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function keeps the learning rate at 0.001 for the first ten epochs\n",
    "# and decreases it exponentially after that.\n",
    "def scheduler(epoch):\n",
    "    if epoch < 10:\n",
    "        return float(beta)\n",
    "    else:\n",
    "        return float(beta * tf.math.exp(0.1 * (10 - epoch)))\n",
    "\n",
    "callback = LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "acc_test_base = np.sum(t_test==1)/t_test.shape[0]\n",
    "print('Baseline test accuracy = {}'.format(acc_test_base))   \n",
    "print(\"==================================================\")\n",
    "print('L = {}, emb_size = {}'.format(L, emb_size))\n",
    "\n",
    "model = thesis_model()\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=beta),\n",
    "              loss= binary_crossentropy,\n",
    "              metrics=['accuracy',\n",
    "                       AUC()\n",
    "                      ]\n",
    "             )\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"./best_model-{epoch:02d}-{loss:.4f}.h5\",\n",
    "                                            monitor=\"val_auc\",\n",
    "                                            verbose=1,\n",
    "                                            save_best_only=True,\n",
    "                                            mode=\"max\",\n",
    "                                            save_freq=\"epoch\"\n",
    "                                            )\n",
    "\n",
    "history = model.fit([x_train[:,:L].astype(int), x_train[:,L:].astype(int)],\n",
    "                    t_train,\n",
    "                    # validation_data=(\n",
    "                    #     [x_valid[:,:L].astype(int), x_valid[:,L:].astype(int)],\n",
    "                    #     t_valid),\n",
    "                    epochs = max_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=1,\n",
    "                    callbacks=[callback,\n",
    "                              #  ReduceLROnPlateau(), \n",
    "                              #  model_checkpoint_callback\n",
    "                               ] \n",
    "                    )\n",
    "\n",
    "# def get_key(keystart, list):\n",
    "#     for k in list:\n",
    "#         if k[:len(keystart)] == keystart:\n",
    "#             return k\n",
    "#     return None\n",
    "\n",
    "# keys = history.history.keys()\n",
    "# key_val_acc = get_key('val_acc', keys)\n",
    "# key_val_auc = get_key('val_auc', keys)\n",
    "# key_acc = get_key('acc', keys)\n",
    "# key_auc = get_key('auc', keys)\n",
    "# plt.figure(figsize=(9,6))\n",
    "# ep = np.arange(1,max_epochs+1)\n",
    "# plt.plot(ep, history.history[key_val_auc], 'r')\n",
    "# plt.xticks(np.arange(0,max_epochs+1,5, dtype=np.int))\n",
    "# plt.plot(ep, history.history[key_auc], 'b')\n",
    "# plt.plot(ep, history.history[key_val_acc], 'r:')\n",
    "# plt.plot(ep, history.history[key_acc], 'b:')\n",
    "# plt.legend(['val.auc', 'auc', 'val.acc', 'acc'])\n",
    "# plt.grid(b=True)\n",
    "# if USE_W2V:\n",
    "#     title=\"L={}, embsize={}, w2v={}, layers={}\".format(\n",
    "#             L, w2v_emb_size, True, num_hidden)\n",
    "# else:\n",
    "#     title=\"L={}, embsize={}, w2v={}, layers={}\".format(\n",
    "#             L, emb_size, False, num_hidden)\n",
    "# # plt.title(title)\n",
    "# plt.show(block=False)\n",
    "# # plt.savefig(cdir + \"./image_1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([x_test[:,:L].astype(int), x_test[:,L:].astype(int)],\n",
    "                t_test,\n",
    "                verbose=1,\n",
    "                batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate mean validation AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = history.history.keys()\n",
    "print(keys)\n",
    "key_val_auc = get_key('val_auc', keys)\n",
    "print(key_val_auc)\n",
    "X = history.history[key_val_auc]\n",
    "print(X)\n",
    "mean_val_auc = np.mean(X)\n",
    "mean_val_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate mean validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = history.history.keys()\n",
    "print(keys)\n",
    "key_val_acc = get_key('val_accuracy', keys)\n",
    "print(key_val_acc)\n",
    "X = history.history[key_val_acc]\n",
    "print(X)\n",
    "mean_val_acc = np.mean(X)\n",
    "mean_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate mean validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = history.history.keys()\n",
    "print(keys)\n",
    "key_val_loss = get_key('val_loss', keys)\n",
    "print(key_val_loss)\n",
    "X = history.history[key_val_loss]\n",
    "print(X)\n",
    "mean_val_loss = np.mean(X)\n",
    "mean_val_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
